<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description" content="PointPatchRL project page">
  <meta name="keywords" content="Reinforcement Learning, Point Clouds, Representation Learning, Masked Reconstruction">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>PointPatchRL</title>

  <script type="text/javascript">
    window.MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']]
      },
      svg: {
        fontCache: 'global'
      }
    };
  </script>
  <script type="text/javascript" id="MathJax-script" async
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/patch_icon.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>

<body>
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-2 publication-title">
              PointPatchRL - Masked Reconstruction Improves Reinforcement Learning on Point Clouds
            </h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href=https://alr.iar.kit.edu/21_527.php target="_blank">Bal√°zs Gyenes</a><sup>1,2</sup>,
              </span>
              <span class="author-block">
                Nikolai Franke<sup>1</sup>,
              </span>
              <span class="author-block">
                <a href=https://alr.iar.kit.edu/21_72.php target=_blank>Philipp Becker</a><sup>1,2</sup>,
              </span>
              <span class="author-block">
                <a href=https://alr.iar.kit.edu/21_65.php target=_blank>Gerhard Neumann</a><sup>1,2</sup>,
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <sup>1</sup>Autonomous Learning Robots (ALR), Karlsruhe Institute of Technology (KIT)
              </span>
              <span class="author-block">
                <sup>2</sup>HIDSS4Health - Helmholtz Information and Data Science School for Health
              </span>
            </div>

            <div class="column has-text-centered">
              <h2 class="title is-4">CoRL 2024 Spotlight</h2>
              <div class="publication-links">
                <!-- OpenReview Link. -->
                <span class="link-block">
                  <a href="https://openreview.net/forum?id=3jNEz3kUSl"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>OpenReview</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="https://arxiv.org/pdf/2410.18800" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>
                <!-- Code Link. -->
                <span class="link-block">
                  <a href="https://github.com/balazsgyenes/pprl"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code (coming soon!)</span>
                  </a>
                </span>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="hero teaser">
    <div class="container is-max-desktop">
      <img src="./static/images/figure_overview.png">
      <!-- Top: We train a patching-based tokenizer and transformer encoder to compute a latent embedding for the RL policy
      and state-action-value estimation using sequence pooling. The
      entire pipeline learns using the critic's gradients while we detach the latent embedding before pro-
      viding it to the actor. Bottom: We augment the policy learning using masked reconstruction. Using
      the token sorting, masking, and transformer decoder introduced by PointGPT, we minimize the
      Chamfer distance for the point's positions and the mean squared reconstruction error for colors. This
      auxiliary loss provides an additional training signal for the shared encoder and tokenizer, improving
      RL performance and sample efficiency. -->
    </div>
    </div>
  </section>

  <!-- Abstract. -->
  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            Perceiving the environment via cameras is crucial for Reinforcement Learning (RL) in robotics.
            While images are a convenient form of representation, they often complicate extracting important geometric
            details, especially with varying geometries or deformable objects.
            In contrast, point clouds naturally represent this geometry and easily integrate color and positional data
            from
            multiple camera views.
            However, while deep learning on point clouds has seen many recent successes, RL on point clouds is
            under-researched, with only the simplest encoder architecture considered in the literature.
            We introduce PointPatchRL (PPRL), a method for RL on point clouds that builds on the common paradigm of
            dividing
            point clouds into overlapping patches, tokenizing them, and processing the tokens with transformers.
            PPRL provides significant improvements compared with other point-cloud processing architectures previously
            used
            for RL.
            We then complement PPRL with masked reconstruction for representation learning and show that our method
            outperforms strong model-free and model-based baselines on image observations in complex manipulation tasks
            containing deformable objects and variations in target object geometry.
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- <section class="section">
    <div class="container is-max-desktop">

      <h2 class="title is-3">Diverse Skill Learning</h2>
      <div class="columns is-centered">
        <div class="column is-full-width">
          <h2 class="title is-4">Automatic Curriculum Learning using Energy-Based per-Expert Context Distribution</h2>

          Figure 1 depicts the sampling procedure of Di-SkilL.During Inference the agent observes contexts
          $\boldsymbol{c}$ from the environment's unknown context
          distribution $p(\boldsymbol{c})$. The agent calculates the gating probabilities $\pi(o|\boldsymbol{c})$ for
          each
          context and samples an expert $o$ resulting in $(o, \boldsymbol{c})$ samples marked in blue. During
          Training we first sample a batch of contexts $\boldsymbol{c}$ from $p(\boldsymbol{c})$, which is used to
          calculate the per-expert context distribution $\pi(\boldsymbol{c}|o)$ for each expert $o = 1,..., K$.
          The $\pi(\boldsymbol{c}|o)$ provides a higher probability for contexts preferred by the expert
          $\pi(\boldsymbol{\theta}|\boldsymbol{c}, o)$. To enable curriculum learning, we provide each expert the
          contexts sampled from its corresponding
          $\pi(\boldsymbol{c}|o)$, resulting in the samples $(o, \boldsymbol{c}_T)$ marked in orange. In both cases,
          the chosen $\pi(\boldsymbol{\theta}|\boldsymbol{c}, o)$ samples motion primitive parameters
          $\boldsymbol{\theta}$ for each context, resulting in
          a trajectory $\tau$ that is subsequently executed on the environment. Before execution, the corresponding
          context, e.g., the goal position of a box, needs to be set in the environment. This is illustrated by the
          dashed arrows, with the context in blue for inference and orange for training.

        </div>
      </div>

      <div class="columns is-centered">
        <div class="column is-full-width">
          <h2 class="title is-4">Specializing and Overlapping Context Distributions lead to Diverse Skills</h2>
          <img src="./static/images/fig2.png" style="width: 80%; height: auto; margin-left: auto; margin-right: auto;
          display: block;">
          <p>
            Figure 2: (a) High-probability regions of the individual per-expert context distributions, where a
            color represents an expert $o$. (b) Number of active experts for context regions.
          </p>
          <br>
          <p>
            The maximum entropy-based objective allows learning diverse skills to the same or similar tasks defined by
            the contexts. For this, the per-expert context distributions need to specialize in a sub region of the
            context context space (a), but at the same time overlapping regions are necessary to learn diverse skills
            to similar tasks (b). These properties are ensured by the decomposed objective (please see the paper).
          </p>
        </div>
      </div>

      <div class="columns is-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Tasks</h2>
          <div class="columns is-multiline">
            <div class="column is-half">
              <h3 class="title is-4">5-Link Reacher</h3>
              <div class="content has-text-justified">
                <p>
                  The 5-Link reacher task is an extension of the classical 2-Link reacher task from OpenAI Gym.
                  The reacher has to reach a goal position with its tip within all quadrants in the context space.
                  A significant challenge in this task is the time-sparse reward that provides only a reward signal
                  at the end of the episode.
                </p>
                <p>
                  The following video shows diverse reaching skills learned by Di-SkilL. The skills were sampled during
                  inference time from the gating distribution.
                </p>
              </div>
              <div class="content has-text-centered">
                <video id="replay-video" autoplay controls muted preload playsinline width="80%">
                  <source src="./static/videos/reacher/reacher.mp4" type="video/mp4">
                </video>
              </div>
              <br />
            </div>
            <div class="column is-half">
              <h3 class="title is-4">Box Pushing with Obstacles</h3>
              <div class="content has-text-justified">
                <p>
                  In the Box Pushing with Obstacle task a 7-DoF robot is tasked to push a box to a target position and
                  rotation while avoiding an obstacle. The 5-dimensional context consists of the box's target
                  position, orientation and the obstacle's position. The task is additionally challenging due to the
                  time-sparse reward structure.
                </p>
                <p>
                  The following video shows diverse reaching skills learned by Di-SkilL. The skills were sampled
                  during inference time from the gating distribution.
                </p>
              </div>
              <div class="content has-text-centered">
                <video id="replay-video" autoplay controls muted preload playsinline width="86%">
                  <source src="./static/videos/bpobs/bpobs.mp4" type="video/mp4">
                </video>
              </div>
              <br />
            </div>

            <h3 class="title is-4">Hopper Jump</h3>
            <div class="content has-text-justified">
              <p>
                The Hopper from OpenAI Gym is tasked to jump as high as possible while landing in a goal position as
                marked by the green and red dots. This task has a non-markovian reward structure which makes learning
                skills with step-based approaches infeasible.
              </p>
              <p>
                The following videos show the behaviors of Di-SkilL's individual experts. We have sampled contexts from
                each per-expert context distribution and exectued the corresponding expert. The goal of the videos
                is to show that each expert is learning different skills.
                This <strong> first expert (left)</strong> builds momentum for the jump by using the first joint and
                stabilizes by
                landing on its foot.
                </br>
                This <strong> second expert (middle) </strong> builds momentum for the jump by using the first joint and
                stabilizes by
                landing on the hopper's "head". The expert is responsible for landing positions that are further away
                from the initial position.
                </br>
                This <strong> third expert (right)</strong> builds momentum for the jump by using the first joint and
                stabilizes by
                landing on the hopper's "head". The expert is responsible for landing positions that are next to the
                initial position.
              </p>
            </div>
            <div class="content has-text-centered">
              <video id="replay-video" autoplay controls muted preload playsinline width="33%">
                <source src="./static/videos/hj/expert1.mp4" type="video/mp4">
              </video>

              <video id="replay-video" autoplay controls muted preload playsinline width="33%">
                <source src="./static/videos/hj/expert2.mp4" type="video/mp4">
              </video>
              <video id="replay-video" autoplay controls muted preload playsinline width="33%">
                <source src="./static/videos/hj/expert3.mp4" type="video/mp4">
              </video>
            </div>
            <br />

            <h3 class="title is-4">Table Tennis</h3>
            <div class="content has-text-justified">
              <p>
                In the table tennis task a 7-degree of freedom (DoF) robot has to learn fast and precise motions to
                smash
                the ball to a desired position on the opponent's side. The 5-dimensional. context consists of the
                incoming
                ball's landing position, the desired landing position on the opponent's side and the ball's initial
                velocity. The table tennis environment requires good exploratory behavior and has a non-makrovian reward
                structure making step-based approaches infeasible to learn usefull skills.
              </p>
              <p>
                The videos blow show diverse striking skills learned by Di-SkilL. For each of the videos the ball's
                landing position on the opponent's side is fixed and the ball's inital landing position and velocity are
                varied. The shown skills correspond to executing the experts sampled from the gatinng distribution
                during
                inference.
              </p>
            </div>
            <div class="content has-text-centered">
              <video id="replay-video" autoplay controls muted preload playsinline width="49%">
                <source src="./static/videos/tt/tt1.mp4" type="video/mp4">
              </video>
              <video id="replay-video" autoplay controls muted preload playsinline width="49%">
                <source src="./static/videos/tt/tt2.mp4" type="video/mp4">
              </video>
              <video id="replay-video" autoplay controls muted preload playsinline width="49%">
                <source src="./static/videos/tt/tt3.mp4" type="video/mp4">
              </video>
              <video id="replay-video" autoplay controls muted preload playsinline width="49%">
                <source src="./static/videos/tt/tt4.mp4" type="video/mp4">
              </video>
            </div>
            <br />
            <h3 class="title is-4">Robot Mini Golf</h3>
            <div class="content has-text-justified">
              <p>
                The 7-DoF robot is tasked to hit the ball in an environment with two obstacles, where the blue obstacle
                is static and the green is reset in each episode. The ball has to pass the tight goal on the other side
                of the table to achieve a success. This environment has a non-markovian reward structure which makes
                learning difficult.
              </p>
              <p>
                The following video shows diverse skills where the goal is fixed and the ball's and the obstacle's
                initial positions are varied. The experts are sampled from the gating distribution during inference.
              </p>
            </div>
            <div class="content has-text-centered">
              <video id="replay-video" autoplay controls muted preload playsinline width="60%">
                <source src="./static/videos/mg/mg.mp4" type="video/mp4">
              </video>
            </div>
            <br />
          </div>
        </div>
      </div>


    </div>
  </section> -->


  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@inproceedings{gyenes2024pointpatchrl,
  title={PointPatch{RL} - Masked Reconstruction Improves Reinforcement Learning on Point Clouds},
  author={Bal{\'a}zs Gyenes and Nikolai Franke and Philipp Becker and Gerhard Neumann},
  booktitle={8th Annual Conference on Robot Learning},
  year={2024},
  url={https://openreview.net/forum?id=3jNEz3kUSl}
}</code></pre>
    </div>
  </section>


  <!--<footer class="footer">-->
  <!--  <div class="container">-->
  <!--    <div class="content has-text-centered">-->
  <!--      <a class="icon-link"-->
  <!--         href="./static/videos/nerfies_paper.pdf">-->
  <!--        <i class="fas fa-file-pdf"></i>-->
  <!--      </a>-->
  <!--      <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>-->
  <!--        <i class="fab fa-github"></i>-->
  <!--      </a>-->
  <!--    </div>-->
  <!--    <div class="columns is-centered">-->
  <!--      <div class="column is-8">-->
  <!--        <div class="content">-->
  <!--          <p>-->
  <!--            This website is licensed under a <a rel="license"-->
  <!--                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative-->
  <!--            Commons Attribution-ShareAlike 4.0 International License</a>.-->
  <!--          </p>-->
  <!--          <p>-->
  <!--            This means you are free to borrow the <a-->
  <!--              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,-->
  <!--            we just ask that you link back to this page in the footer.-->
  <!--            Please remember to remove the analytics code included in the header of the website which-->
  <!--            you do not want on your website.-->
  <!--          </p>-->
  <!--        </div>-->
  <!--      </div>-->
  <!--    </div>-->
  <!--  </div>-->
  <!--</footer>-->

</body>

</html>